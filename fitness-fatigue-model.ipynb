{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitness - fatigue modelling with Banister\n",
    "====\n",
    "\n",
    "This notebook deals with the Banister models \\cite{banister1975systems, busso1994} on the evaluation and prediction of athlete performance in function of fatigue and fitness. The general equation is given below:\n",
    "\n",
    "\\begin{equation}\n",
    "p_{t}=p_{0}+k_{a} \\sum_{s=0}^{t-1} e^{-(t-s) / \\tau_{a}} w_{s}-k_{f} \\sum_{s=0}^{t-1} e^{-(t-s) / \\tau_{f}} w_s\n",
    "\\label{eq:banister}\n",
    "\\end{equation}\n",
    "\n",
    "* $p_{t}$ is the modelled performance at time $t$\n",
    "* $p_0$ is the initial performance level\n",
    "* $k_a$ and $k_f$ are fitness and fatigue magnitude factor\n",
    "* $\\tau_a$ and $\\tau_f$ are fitness and fatigue time decay\n",
    "* $w_s$ is the known training load per time unit, or ' the known training load per week (or day) from the first week of training to the week (or day) preceding the performance'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are however some ***issues with the Banister model*** which are described in detail in \\cite{Hellard2006}.\n",
    "1. The model parameters suffer from poor identifiability, which seems logical as the mechanisms could correct for eachother. This feature leads to a lot of variability in the resulting parameter estimates. The model somehow has been reported to predict reasonably well. The model does have 4 parameters that can bring this in. \n",
    "2. On top of that there is usually only limited data, between 10 and 20 data points per athlete, to fit the model.\n",
    "3. The Banister model parameters might not be so physically accurate, the model plausibility could be low.\n",
    "\n",
    "There appears to be the upside that the model is stable and parameter estimates do not change much (on top of the other variability?) when a data point is left out.\n",
    "\n",
    "\n",
    "Things to check on \\cite{Hellard2006} still:\n",
    "1. how was the model prediction assessed?\n",
    "2. how was bootstrapping used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Python packages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-09T21:26:25.624Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "#basic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from matplotlib import cm\n",
    "from palettable.scientific import sequential\n",
    "from palettable import tableau\n",
    "# calculations\n",
    "from scipy.optimize import brute, differential_evolution\n",
    "import multiprocessing\n",
    "from pyswarm import pso\n",
    "# sensitivity analysis\n",
    "from SALib.analyze import sobol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-09T21:26:25.627Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# naming of parameters in plots\n",
    "parname_dict = {\n",
    "    'k_a' : '$k_a$',\n",
    "    'k_f' : '$k_f$',\n",
    "    'tau_a' : r'$\\tau_a$',\n",
    "    'tau_f' : r'$\\tau_f$',\n",
    "    'p_0' : '$p_0$',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-09T21:26:25.629Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# overall plot settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['axes.labelsize'] = 16\n",
    "plt.rcParams['axes.titlesize'] = 18\n",
    "plt.rcParams['legend.fontsize'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-09T21:26:25.632Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# get all sporter dataset names\n",
    "dataset_names = [f for f in os.listdir() if 'modellen.xlsx' in f]\n",
    "sheetnames = ['Edwards', 'Banister', 'Lucia', 'sRPE', 'TSS']\n",
    "dataset_dict = dict() #dictionary to store all data in\n",
    "for d in dataset_names:\n",
    "    # read sporter excel file and store in dictionary, referenced by sporter name\n",
    "    dataset_dict[d[:-14]] = dict()\n",
    "    for s in sheetnames:\n",
    "        dataset_dict[d[:-14]][s] = pd.read_excel(d, sheet_name=s, index_col=0).loc[:,\n",
    "                   ['Datum', 'TL', 'TT (W)', 'PTE', 'Fitness', 'NTE', 'Fatigue',\n",
    "                   'TT performance', 'Performance', 'Modelfit']] #only store columns of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-09T21:26:25.637Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "#create figure\n",
    "fig, ax  = plt.subplots(figsize=(12,3))\n",
    "#select dataset\n",
    "sporter = 'Wietse'\n",
    "datatype = 'Lucia'\n",
    "data2plot = dataset_dict[sporter][datatype]\n",
    "#plot data\n",
    "ax.plot(data2plot.loc[:,'TT (W)'],'*', label='Data '+sporter)\n",
    "#format the figure\n",
    "ax.set_xlim(0,100);\n",
    "ax.set_xlabel('Time, in days');\n",
    "ax.set_ylabel('Performance,\\nin arbitrary units');\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Banister model\n",
    "\n",
    "\\begin{equation}\n",
    "p_{t}=p_{0}+k_{a} \\sum_{s=0}^{t-1} e^{-(t-s) / \\tau_{a}} w_{s}-k_{f} \\sum_{s=0}^{t-1} e^{-(t-s) / \\tau_{f}} w_s\n",
    "\\label{eq:banister2}\n",
    "\\end{equation}\n",
    "\n",
    "Below is the implementation for Eq.\\ref{eq:banister}. *The same notation is used, thus the meaning of the inputs to the model function* `ff_banister` *can be found under Eq.\\ref{eq:banister}*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-09T21:26:25.642Z"
    },
    "code_folding": [
     0,
     9,
     22
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def sum_exp(t, k, tau, w_s):\n",
    "    \"\"\"calculation for summation exponential term (is this a convolution?)\n",
    "    needed for the calculation of the fitness and fatigue terms\"\"\"\n",
    "    retval = np.zeros((len(t),1))\n",
    "    for i in range(1,len(t)-1):\n",
    "        retval[i] = np.sum(np.exp(-(i-np.arange(i+1))/tau)*w_s[:i+1])\n",
    "        \n",
    "    return k*retval\n",
    "\n",
    "def ff_banister(t, p_0, k_a, k_f, tau_a, tau_f, w_t, tt_W):\n",
    "    \"\"\"\n",
    "    returns the performance at time t calculated with all the input parameters\n",
    "    \"\"\"\n",
    "    p_t = p_0 + sum_exp(t, k_a, tau_a, w_t) - sum_exp(t, k_f, tau_f, w_t)\n",
    "    \n",
    "    for ti, tv in enumerate(tt_W):\n",
    "        if tv>0.1:\n",
    "            p_t[ti] = p_t[ti] + 2*(k_f-k_a)*w_t[ti-1]\n",
    "            print(ti)\n",
    "    \n",
    "    return p_t\n",
    "\n",
    "def kobes_way(t, p_0, k_a, k_f, tau_a, tau_f, w_t, tt_W):\n",
    "    \"\"\"\n",
    "    try exactly the  code from the excel file\n",
    "    \"\"\"\n",
    "    retval = np.zeros((len(t),1))\n",
    "    retval[0] = p_0\n",
    "    pte = np.zeros((len(t),1))\n",
    "    nte = np.zeros((len(t),1))\n",
    "    \n",
    "    for i in range(1,len(t)):\n",
    "        pte[i] = (pte[i-1]*np.exp(-1/tau_a) + w_t[i])\n",
    "        nte[i] = (nte[i-1]*np.exp(-1/tau_f) + w_t[i])\n",
    "        if tt_W[i]<0.1:\n",
    "            retval[i] = p_0 + (pte[i])*k_a - (nte[i])*k_f\n",
    "        else:\n",
    "            retval[i] = p_0 + (pte[i-1]*np.exp(-1/tau_a))*k_a - (nte[i-1]*np.exp(-1/tau_f))*k_f\n",
    "            \n",
    "    simres = np.array((retval, pte, nte)).reshape(3, len(t)).transpose()\n",
    "        \n",
    "    return pd.DataFrame(simres, index=t, columns=['overall', 'fit', 'fat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving data\n",
    "\n",
    "Select an athlete out of Thibaux, Wietse or Tom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-09T21:26:25.647Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# model inputs\n",
    "sporter = 'Thibaux'\n",
    "data4mod = dataset_dict[sporter]['TSS']\n",
    "t = data4mod.index.values\n",
    "data4mod[['TT (W)']] = data4mod[['TT (W)']].fillna(value=0)\n",
    "data4mod[['TL']] = data4mod[['TL']].fillna(value=0)\n",
    "w_t = data4mod[['TL']].values.ravel()\n",
    "tt_w = data4mod[['TT (W)']].values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying model implementation\n",
    "\n",
    "Checking whether simulated outcomes are the same as those of Kobe V."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-09T21:26:25.649Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# simulate the model using parameters from the excel and compare with Kobe's dank excel code\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "mod_out = kobes_way(t, 240, 0.592627747, 0.830655943, 11.23205693, 6.327383027, w_t, tt_w)\n",
    "plt.plot(t, data4mod.loc[:,'Modelfit'], label='Kobe')\n",
    "plt.plot(mod_out['overall'] , '--', label='Michael')\n",
    "# plt.plot(t, data4mod.loc[:,'TL'], '*', label='TL')\n",
    "#plot data\n",
    "ax.plot(t[tt_w>0.1], tt_w[tt_w>0.1],'*', label='Data '+sporter)\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark and stop model\n",
    "\n",
    "Or 'prikkel' and 'stop' model: calculation of the ideal times to respectively have the last big exercise and when to stop exercising prior to a contest (when the athlete should peak). Need to check this\n",
    "\n",
    "\n",
    "\n",
    "This is calculated based on the fitter Banister model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-09T21:26:25.652Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# the model\n",
    "def spark_stop(pars):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "        - pars {list}: contains the parameter values of the Banister model in the following order:\n",
    "        [k_a, k_f, tau_a, tau_f]\n",
    "    \"\"\"\n",
    "    # parameter values\n",
    "    [k_a, k_f, tau_a, tau_f] = pars    \n",
    "    spark = (tau_a*tau_f/(tau_a - tau_f)) * np.log((k_f*tau_a)/(k_a*tau_f))\n",
    "    stop = (tau_a*tau_f/(tau_a - tau_f)) * np.log(k_f/k_a)\n",
    "    return spark, stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_stop([0.634,0.716,9.541,7.812])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SALib.analyze import sobol, delta\n",
    "from SALib.sample import saltelli, latin\n",
    "from SALib.plotting import bar, morris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model inputs\n",
    "problem = {\n",
    "    'num_vars': 4,\n",
    "    'names': ['k_a', 'k_f', 'tau_a', 'tau_f'],\n",
    "    'bounds': [[0, 3],[0, 3],[0, 60],[0, 60]]\n",
    "}\n",
    "\n",
    "# Generate samples\n",
    "param_values = latin.sample(problem, 10000)\n",
    "\n",
    "# Run model (example)\n",
    "Y = np.array([spark_stop(p) for p in param_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform analysis\n",
    "Y_in = Y[:,1].copy()\n",
    "good_values = ~((sobol_in<0) | (np.isinf(sobol_in)) | (np.isnan(sobol_in)))\n",
    "Y_in = Y_in[good_values] \n",
    "X_in = param_values[good_values,:]\n",
    "Si = delta.analyze(problem, X_in, Y_in, print_to_console=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation\n",
    "Visualise objective function and conduct a global optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-09T21:26:25.654Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# objective function\n",
    "def mod_err_banister(pars, t_loc, w_t_loc, tt_w_loc, p_0, err_type='sse'):\n",
    "    \"\"\"\n",
    "    pars {list}: contains the parameter values of the Banister model in the following order:\n",
    "        [k_a, k_f, tau_a, tau_f]\n",
    "    t_loc {np.array 1D}: time denotation (in days)\n",
    "    w_t_loc {np.array 1D}: values from TL column in data\n",
    "    tt_w_loc {np.array 1D}: values from TT (W) column in data\n",
    "    err_type {string}: specifies model error calculation type, standard sum of squared errors\n",
    "    \"\"\"\n",
    "    # parameter values\n",
    "    [k_a, k_f, tau_a, tau_f] = pars\n",
    "    # calculate model output\n",
    "    mod_out = kobes_way(t_loc, p_0, k_a, k_f, tau_a, tau_f, w_t_loc, tt_w_loc)\n",
    "    # reformat data for model error calculation\n",
    "    data2compare = tt_w_loc.copy()\n",
    "    data2compare[tt_w_loc<0.1] = np.nan\n",
    "    # model error calculation\n",
    "    if err_type == 'sse': #sum of squared errors\n",
    "        mod_err = np.nansum(np.square(mod_out.overall.values - data2compare))\n",
    "        \n",
    "    return mod_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brute force is carried out, which checks the model fit at a range of specified parameter value combinations. This is mainly to visualise the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-09T21:26:25.656Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# parameter ranges \n",
    "par_ranges = (slice(0, 3, 0.1), slice(0, 3, 0.1),\n",
    "             slice(0, 60, 1), slice(0,60,1))\n",
    "\n",
    "par_ranges_dict = {\n",
    "    'k_a' : np.arange(1e-4,3,step=0.1),\n",
    "    'k_f' : np.arange(1e-4,3,step=0.1),\n",
    "    'tau_a' : np.arange(0,60,step=1),\n",
    "    'tau_f' : np.arange(0,60,step=1),\n",
    "}\n",
    "\n",
    "lowbounds = [l[0] for l in list(par_ranges_dict.values())]\n",
    "uppbounds = [l[-1] for l in list(par_ranges_dict.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = brute(mod_err_banister, par_ranges, args=(t, w_t, tt_w), full_output=True, disp=True)#, workers=32)\n",
    "xeval = results[2]\n",
    "feval = results[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# how to plot this\n",
    "\n",
    "fig_brutres = plt.figure(figsize=(10,7), constrained_layout=True)\n",
    "kleurmap = cm.viridis_r #sequential.Bilbao_10.mpl_colormap\n",
    "cont_nlev = 100\n",
    "\n",
    "spec = gridspec.GridSpec(ncols=41, nrows=2, figure=fig_brutres)\n",
    "ax11 = fig_brutres.add_subplot(spec[0, :20])\n",
    "ax12 = fig_brutres.add_subplot(spec[0, 20:-1])\n",
    "ax21 = fig_brutres.add_subplot(spec[1, :20])\n",
    "ax22 = fig_brutres.add_subplot(spec[1, 20:-1])\n",
    "ax_cb = fig_brutres.add_subplot(spec[:,-1])\n",
    "# we plot each optimal objective function value for a combination of two selected parameters\n",
    "# normalising to the same colour scale\n",
    "normF = plt.Normalize(20, 100)\n",
    "\n",
    "# plot optimal value found in function of combination of values of two parameters\n",
    "# right now I have no idea but to rewrite this manually\n",
    "# bc changing the parameter selection also changes the indexing\n",
    "\n",
    "# k_a vs k_f\n",
    "dims = [0,1]\n",
    "parnames = [list(par_ranges_dict.keys())[i] for i in dims]\n",
    "disc1 = np.shape(feval)[dims[0]]\n",
    "disc2 = np.shape(feval)[dims[1]]\n",
    "# storage of parameter value combinations\n",
    "p1 = par_ranges_dict[parnames[0]]\n",
    "p2 = par_ranges_dict[parnames[1]]\n",
    "P1, P2 = np.meshgrid(p1, p2)\n",
    "F = P1*2 +P2\n",
    "\n",
    "#loop over first variable\n",
    "for i in range(disc1):\n",
    "    #loop over second variable\n",
    "    for j in range(disc2):\n",
    "        F[j,i] = np.sqrt(np.min(feval[i,j,:,:]))\n",
    "\n",
    "# plot contour\n",
    "mappab = ax11.contourf(P1, P2, F, levels=cont_nlev, norm=normF, cmap=kleurmap)\n",
    "ax11.set_xlabel(parname_dict[parnames[0]])\n",
    "ax11.set_ylabel(parname_dict[parnames[1]]);\n",
    "\n",
    "# tau\n",
    "dims = [2,3]\n",
    "parnames = [list(par_ranges_dict.keys())[i] for i in dims]\n",
    "disc1 = np.shape(feval)[dims[0]]\n",
    "disc2 = np.shape(feval)[dims[1]]\n",
    "# storage of parameter value combinations\n",
    "p1 = par_ranges_dict[parnames[0]]\n",
    "p2 = par_ranges_dict[parnames[1]]\n",
    "P1, P2 = np.meshgrid(p1, p2)\n",
    "F = P1*2 +P2\n",
    "\n",
    "#loop over first variable\n",
    "for i in range(disc1):\n",
    "    #loop over second variable\n",
    "    for j in range(disc2):\n",
    "        F[j,i] = np.sqrt(np.min(feval[:,:,i,j]))\n",
    "\n",
    "# plot contour\n",
    "ax12.contourf(P1, P2, F, levels=cont_nlev, norm=normF, cmap=kleurmap)\n",
    "ax12.set_xlabel(parname_dict[parnames[0]])\n",
    "ax12.set_ylabel(parname_dict[parnames[1]]);\n",
    "\n",
    "\n",
    "# fitness\n",
    "dims = [0,2]\n",
    "parnames = [list(par_ranges_dict.keys())[i] for i in dims]\n",
    "disc1 = np.shape(feval)[dims[0]]\n",
    "disc2 = np.shape(feval)[dims[1]]\n",
    "# storage of parameter value combinations\n",
    "p1 = par_ranges_dict[parnames[0]]\n",
    "p2 = par_ranges_dict[parnames[1]]\n",
    "P1, P2 = np.meshgrid(p1, p2)\n",
    "F = P1*2 +P2\n",
    "\n",
    "#loop over first variable\n",
    "for i in range(disc1):\n",
    "    #loop over second variable\n",
    "    for j in range(disc2):\n",
    "#         print(j)\n",
    "        F[j,i] = np.sqrt(np.min(feval[i,:,j,:]))\n",
    "\n",
    "# plot contour\n",
    "ax21.contourf(P1, P2, F, levels=cont_nlev, norm=normF, cmap=kleurmap)\n",
    "ax21.set_xlabel(parname_dict[parnames[0]])\n",
    "ax21.set_ylabel(parname_dict[parnames[1]]);\n",
    "\n",
    "# fatigue\n",
    "dims = [1,3]\n",
    "parnames = [list(par_ranges_dict.keys())[i] for i in dims]\n",
    "disc1 = np.shape(feval)[dims[0]]\n",
    "disc2 = np.shape(feval)[dims[1]]\n",
    "# storage of parameter value combinations\n",
    "p1 = par_ranges_dict[parnames[0]]\n",
    "p2 = par_ranges_dict[parnames[1]]\n",
    "P1, P2 = np.meshgrid(p1, p2)\n",
    "F = P1*2 +P2\n",
    "\n",
    "#loop over first variable\n",
    "for i in range(disc1):\n",
    "    #loop over second variable\n",
    "    for j in range(disc2):\n",
    "#         print(j)\n",
    "        F[j,i] = np.sqrt(np.min(feval[:,i,:,j]))\n",
    "\n",
    "# plot contour\n",
    "ax22.contourf(P1, P2, F, levels=cont_nlev, norm=normF, cmap=kleurmap)\n",
    "ax22.set_xlabel(parname_dict[parnames[0]])\n",
    "ax22.set_ylabel(parname_dict[parnames[1]]);\n",
    "\n",
    "\n",
    "# colorbar\n",
    "cb = fig_brutres.colorbar(mappab, cax=ax_cb)\n",
    "cb.set_label('absolute model error', fontsize=16)\n",
    "\n",
    "fig_brutres.savefig(sporter+'_brutres.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global optimisation for every exercise data collection method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Particle swarm optimisation\n",
    "The [particle swarm optimisation (PSO) technique](https://en.wikipedia.org/wiki/Particle_swarm_optimization) is a strategy for global optimisation. It is easy to conduct in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-09T21:26:25.658Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# function for inequality contraints on the optimisation\n",
    "# in our case, k_f >= k_a\n",
    "# make sure to have the exact same input arguments as in the optimisation function\n",
    "\n",
    "def kcon(x, t, wt, ttw, p0):\n",
    "    k_a = x[0]\n",
    "    k_f = x[1]\n",
    "    tau_a = x[2]\n",
    "    tau_f = x[3]\n",
    "    # calculate model so that fatigue does not go below 0\n",
    "    mod_out = kobes_way(t, p0, k_a, k_f, tau_a, tau_f, wt, ttw)\n",
    "    \n",
    "    return [k_f-k_a, k_f*tau_f-1e-4, 0.5-np.sum(mod_out.overall.values<0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct global optimisations with PSO per datatype available per athlete. It is even possible to perform a few optimisations per case, in order to increase the chance of finding optimal parameter values and to see which at which datatype the optimisation results in adequate and consistent optimal parameter values most often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-09T17:51:28.781Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# conduct the PSO and store results in a python dictionary\n",
    "dict_optires_pso = dict()\n",
    "n_opts=8 # amount of optimisations to attempt per case\n",
    "for si, sporter in enumerate(['Brecht']): #list(dataset_dict.keys())\n",
    "    dict_optires_pso[sporter]=dict()\n",
    "    for s in sheetnames:\n",
    "        data4mod = dataset_dict[sporter][s]\n",
    "        t = data4mod.index.values\n",
    "        data4mod[['TT (W)']] = data4mod[['TT (W)']].fillna(value=0)\n",
    "        data4mod[['TL']] = data4mod[['TL']].fillna(value=0)\n",
    "        w_t = data4mod[['TL']].values.ravel()\n",
    "        tt_w = data4mod[['TT (W)']].values.ravel()\n",
    "        p0 = dataset_dict[sporter][s]['Performance'][0]\n",
    "        dict_optires_pso[sporter][s] = pd.DataFrame(index = np.arange(n_opts), columns=list(par_ranges_dict.keys())+['f_opt'])\n",
    "        \n",
    "        # conduct a few optimisations\n",
    "        for o in range(n_opts):\n",
    "            pso_x, pso_f = pso(mod_err_banister, lowbounds, uppbounds, ieqcons=[], f_ieqcons=kcon, args=(t, w_t, tt_w, p0), kwargs={},\n",
    "                   swarmsize=1280, omega=0.5, phip=0.5, phig=0.5, maxiter=50, minstep=1e-8, minfunc=1e-8, debug=False, processes=32)\n",
    "            dict_optires_pso[sporter][s].iloc[o,:-1] = pso_x\n",
    "            dict_optires_pso[sporter][s].iloc[o,-1] = pso_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T21:02:06.064588Z",
     "start_time": "2020-05-09T21:02:05.936977Z"
    }
   },
   "outputs": [],
   "source": [
    "# format results as a pandas dataframe\n",
    "mind_optires_pso = pd.MultiIndex.from_product([list(dict_optires_pso.keys()), sheetnames, np.arange(n_opts)])\n",
    "df_optires_pso = pd.DataFrame(index=mind_optires_pso, columns=list(par_ranges_dict.keys())+['f_opt'])\n",
    "\n",
    "# get all optimisation results from the relevant dictionary\n",
    "for sporter in list(dict_optires_pso.keys()):\n",
    "    for s in sheetnames:\n",
    "        df_optires_pso.loc[(sporter, s),:] = dict_optires_pso[sporter][s].values\n",
    "\n",
    "# calculate absolute error, assuming model objective function was SSE\n",
    "df_optires_pso.loc[:,'abs_err'] = (df_optires_pso.loc[:,'f_opt'].values)**(1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Effect on spark and stop time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T21:02:14.592023Z",
     "start_time": "2020-05-09T21:02:14.549777Z"
    }
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "# df_optires_pso_rd = pd.read_pickle('optires_pso.pkl')\n",
    "\n",
    "df_optires_pso_rd = df_optires_pso.copy()\n",
    "df_optires_pso_rd.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T21:02:19.335962Z",
     "start_time": "2020-05-09T21:02:19.289816Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate prikkel and stop for every optimisation result\n",
    "n_opt_tot = len(df_optires_pso_rd)\n",
    "sparklist, stoplist = np.zeros((n_opt_tot,1)), np.zeros((n_opt_tot,1))\n",
    "for r in range(n_opt_tot):\n",
    "    try:\n",
    "        spark, stop = spark_stop(df_optires_pso_rd.iloc[r,:4])\n",
    "    except:\n",
    "        spark, stop = np.nan, np.nan\n",
    "    sparklist[r] = spark\n",
    "    stoplist[r] = stop\n",
    "\n",
    "df_optires_pso_rd.loc[:,'prikkel'] = sparklist  \n",
    "df_optires_pso_rd.loc[:,'stop'] = stoplist   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pickle\n",
    "with open('opti_results_pso_20200505.pkl', 'wb') as f:\n",
    "    pickle.dump(df_optires_pso_rd, f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T09:40:40.249109Z",
     "start_time": "2020-05-08T09:40:38.582099Z"
    }
   },
   "source": [
    "df_optires_pso_rd.to_excel('opti_results_pso_cstrn_20200508.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T21:03:47.198477Z",
     "start_time": "2020-05-09T21:03:47.144407Z"
    }
   },
   "outputs": [],
   "source": [
    "df_optires_pso_rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T21:02:46.987371Z",
     "start_time": "2020-05-09T21:02:46.578776Z"
    }
   },
   "outputs": [],
   "source": [
    "datur = pd.read_excel('Brecht_modellen.xlsx', sheet_name='Overzicht parameters', skiprows=2, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T21:23:01.541123Z",
     "start_time": "2020-05-09T21:23:01.024733Z"
    }
   },
   "outputs": [],
   "source": [
    "# model inputs\n",
    "sporter = 'Brecht'\n",
    "datatype = 'TSS'\n",
    "opt_i = 2\n",
    "\n",
    "#retrieve data\n",
    "data4mod = dataset_dict[sporter][datatype]\n",
    "t = data4mod.index.values\n",
    "data4mod[['TT (W)']] = data4mod[['TT (W)']].fillna(value=0)\n",
    "data4mod[['TL']] = data4mod[['TL']].fillna(value=0)\n",
    "w_t = data4mod[['TL']].values.ravel()\n",
    "tt_w = data4mod[['TT (W)']].values.ravel()\n",
    "p0 = dataset_dict[sporter][datatype]['Performance'][0]\n",
    "# create figure\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "plt.plot(t, data4mod.loc[:,'Modelfit'], ':', label='result Kobe') # Kobe's result\n",
    "kleuren = tableau.Tableau_10.mpl_colors\n",
    "selection = [3,5,7] #range(8)\n",
    "# plot optimisation results\n",
    "for opt_i in selection: #range(n_opts):\n",
    "    # parameter values\n",
    "    optres_loc = df_optires_pso_rd.loc[sporter, datatype, opt_i]\n",
    "    [k_a,k_f,tau_a,tau_f] = optres_loc.loc[['k_a','k_f', 'tau_a', 'tau_f']].values\n",
    "    kleur = kleuren[opt_i]\n",
    "    # line below: get parameters from data\n",
    "    # [tau_a,tau_f,k_a,k_f] = datur.loc[datatype, ['t1','t2','k1','k2']].values\n",
    "\n",
    "    mod_out = kobes_way(t, p0, k_a, k_f, tau_a, tau_f, w_t, tt_w)\n",
    "    plt.plot(mod_out['overall'] , '--', color=kleur, label='result Michael '+str(opt_i), lw=3)\n",
    "#     plt.plot(mod_out['fit'] , '-.', color=kleur, label='Fitness'+str(opt_i), lw=1)\n",
    "#     plt.plot(mod_out['fat'] , ':', color=kleur, label='Fatigue'+str(opt_i), lw=1)\n",
    "\n",
    "#plot data\n",
    "ax.plot(t[tt_w>0.1], tt_w[tt_w>0.1],'*', label='Data '+sporter, markersize=10)\n",
    "plt.legend(loc='upper right');\n",
    "df_optires_pso_rd.loc[sporter, datatype, selection][['k_a', 'k_f','tau_a','tau_f','abs_err']]\n",
    "# plt.ylim(-10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wishlist\n",
    "\n",
    "* model that relates parameters to optimal training time - model sensitivity\n",
    "* validation: \n",
    "    * apply average model (average of fitted parameters?) to all datasets\n",
    "    * leave one out (opletten welke punten)\n",
    "* measurement method: check results global optimisation\n",
    "    * consistency over sporters (tau should be similar for sporters)\n",
    "    \n",
    "    \n",
    "**To do**\n",
    "* refine grid: request however not possible, lowering one tenth... otherwise 324 trillion simulations\n",
    "* Banister model global output for each method + effect on stop and spark time\n",
    "    * global optimisation each method\n",
    "    * sensitivity analysis on stop/spark time outcome\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "(<a id=\"cit-banister1975systems\" href=\"#call-banister1975systems\">Banister, Calvert <em>et al.</em>, 1975</a>) Banister E W, Calvert T W, Savage M V <em>et al.</em>, ``_A systems model of training for athletic performance_'', Aust J Sport Med, vol. 7, number 3, pp. 57--61,  1975.\n",
    "\n",
    "(<a id=\"cit-busso1994\" href=\"#call-busso1994\">Busso, Candau <em>et al.</em>, 1994</a>) Busso Thierry, Candau Robin and Lacour Jean Ren{\\'{e}}, ``_Fatigue and fitness modelled from the effects of training on performance_'', Eur J Appl Physiol Occup Physiol, vol. 69, number 1, pp. 50--54, jan 1994.  [online](https://www.researchgate.net/publication/15242395)\n",
    "\n",
    "(<a id=\"cit-Hellard2006\" href=\"#call-Hellard2006\">Hellard, Avalos <em>et al.</em>, 2006</a>) Hellard Philippe, Avalos Marta, Lacoste Lucien <em>et al.</em>, ``_Assessing the limitations of the Banister model in monitoring training_'', J Sports Sci, vol. 24, number 5, pp. 509--520,  2006.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python3 Michael",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "275px",
    "width": "300px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
